{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\English_paper'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('C:/English_paper')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"'m\", \"am\", text)\n",
    "    text = re.sub(r\"\\'re\", \"are\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'till\", \"until\", text)\n",
    "    text = re.sub('du n no', \"do n't know\", text)\n",
    "    text = re.sub('gon na', 'going to', text)\n",
    "    text = re.sub('cos', 'because', text)\n",
    "    text = re.sub(r\"[()\\#/_@;:<>{}`+=~|]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(texted):\n",
    "    ''' change wrong lemmatized words into correct one'''\n",
    "    \n",
    "    error=[\"wa\",\"are\",\"ha\",\"am\"]\n",
    "    if texted in error:\n",
    "        texted = re.sub(\"wa\",\"be\",texted)\n",
    "        texted = re.sub(\"are\",\"be\",texted)\n",
    "        texted = re.sub(\"ha\",\"have\",texted)\n",
    "        texted = re.sub(\"cos\",\"because\",texted)\n",
    "        texted = re.sub(\"am\",\"be\",texted)\n",
    "        \n",
    "    return texted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IL001.txt', 'IL002.txt', 'IL003.txt', 'IL004.txt', 'IL005.txt', 'IL006.txt', 'IL007.txt', 'IL008.txt', 'IL009.txt', 'IL010.txt', 'IL011.txt', 'IL012.txt', 'IL013.txt', 'IL014.txt', 'IL015.txt', 'IL016.txt', 'IL017.txt', 'IL018.txt', 'IL019.txt', 'IL020.txt', 'IR001.txt', 'IR002.txt', 'IR003.txt', 'IR004.txt', 'IR005.txt', 'IR006.txt', 'IR007.txt', 'IR008.txt', 'IR009.txt', 'IR010.txt']\n"
     ]
    }
   ],
   "source": [
    "'''call the corpus'''\n",
    "file_text=[]\n",
    "for file_root, dirs, files in os.walk('IELTS'):\n",
    "    print(files)\n",
    "    for fname in files:\n",
    "        file_name=os.path.join(file_root,fname)\n",
    "        text=open(file_name,encoding='utf-8').readlines()\n",
    "        file_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/gartl/Documents/MWE level list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''call MWE lemma list.'''\n",
    "BNC_phrase=open('C:/Users/gartl/jupyter_mycode/data folder/spokenBNC_MWE_lemmalist.txt',encoding='utf-8').readlines()\n",
    "BNC_phrase=[i.strip() for i in BNC_phrase]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''sort MWEs in a descending order'''\n",
    "BNC=nltk.FreqDist(BNC_phrase)\n",
    "phrase_lists=[]\n",
    "for i in BNC.most_common():\n",
    "    phrase_lists.append(i[0])\n",
    "#len([i for i in phrase_lists if i[1]>130])\n",
    "#len([i for i in phrase_lists if i[1]>1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you know', 'i mean', \"i do n't know\", 'go to', 'i know', 'sort of', 'do it', 'kind of', 'as well', 'use to']\n"
     ]
    }
   ],
   "source": [
    "'''divide MWE list by several levels by using cut-off point(index) which is obtained by calculating per million score of every thousandth (1000th, 2000th etc) word's frequency. \n",
    "(sorted by frequency in a descending order)'''\n",
    "phrase=[i.replace('_',' ') for i in phrase_lists]\n",
    "phrase=[i.strip() for i in phrase]\n",
    "level1=phrase[:1068]\n",
    "level2=phrase[1068:1674]\n",
    "level3=phrase[1674:2269]\n",
    "level4=phrase[2269:2761]\n",
    "level5=phrase[2761:3194]\n",
    "level6=phrase[3194:3590]\n",
    "#level7=phrase_list[3590:3978]\n",
    "#level8=phrase_list[3978:4281]\n",
    "#level9=phrase_list[4281:4573]\n",
    "#level10=phrase_list[4573:4783]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fall into line with', 'suck me suck me into', 'lumber us with', 'them rock', 'right here and now', 'look that up', 'how come', 'their decline years', 'pull that together', 'pull this out of the bag']\n"
     ]
    }
   ],
   "source": [
    "phrase=open('lemma_MWE_list.txt','r',encoding='utf-8').readlines()\n",
    "phrase=[i.strip() for i in phrase]\n",
    "print(phrase[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "level1=open('MWE_level1.txt','r',encoding='utf-8').readlines()\n",
    "level1=[i.strip().split('\\t')[0].replace('_',' ') for i in level1]\n",
    "\n",
    "level2=open('MWE_level2.txt','r',encoding='utf-8').readlines()\n",
    "level2=[i.strip().split('\\t')[0].replace('_',' ') for i in level2]\n",
    "\n",
    "level3=open('MWE_level3.txt','r',encoding='utf-8').readlines()\n",
    "level3=[i.strip().split('\\t')[0].replace('_',' ') for i in level3]\n",
    "\n",
    "level4=open('MWE_level4.txt','r',encoding='utf-8').readlines()\n",
    "level4=[i.strip().split('\\t')[0].replace('_',' ') for i in level4]\n",
    "\n",
    "level5=open('MWE_level5.txt','r',encoding='utf-8').readlines()\n",
    "level5=[i.strip().split('\\t')[0].replace('_',' ') for i in level5]\n",
    "\n",
    "level6=open('MWE_level6.txt','r',encoding='utf-8').readlines()\n",
    "level6=[i.strip().split('\\t')[0].replace('_',' ') for i in level6]\n",
    "\n",
    "level7=open('MWE_level7.txt','r',encoding='utf-8').readlines()\n",
    "level7=[i.strip().split('\\t')[0].replace('_',' ') for i in level7]\n",
    "\n",
    "level8=open('MWE_level8.txt','r',encoding='utf-8').readlines()\n",
    "level8=[i.strip().split('\\t')[0].replace('_',' ') for i in level8]\n",
    "\n",
    "level9=open('MWE_level9.txt','r',encoding='utf-8').readlines()\n",
    "level9=[i.strip().split('\\t')[0].replace('_',' ') for i in level9]\n",
    "\n",
    "level10=open('MWE_level10.txt','r',encoding='utf-8').readlines()\n",
    "level10=[i.strip().split('\\t')[0].replace('_',' ') for i in level10]\n",
    "\n",
    "level11=open('MWE_level11.txt','r',encoding='utf-8').readlines()\n",
    "level11=[i.strip().split('\\t')[0].replace('_',' ') for i in level11]\n",
    "\n",
    "level12=open('MWE_level12.txt','r',encoding='utf-8').readlines()\n",
    "level12=[i.strip().split('\\t')[0].replace('_',' ') for i in level12]\n",
    "\n",
    "level13=open('MWE_level13.txt','r',encoding='utf-8').readlines()\n",
    "level13=[i.strip().split('\\t')[0].replace('_',' ') for i in level13]\n",
    "\n",
    "level14=open('MWE_level14.txt','r',encoding='utf-8').readlines()\n",
    "level14=[i.strip().split('\\t')[0].replace('_',' ') for i in level14]\n",
    "\n",
    "level15=open('MWE_level15.txt','r',encoding='utf-8').readlines()\n",
    "level15=[i.strip().split('\\t')[0].replace('_',' ') for i in level15]\n",
    "\n",
    "level16=open('MWE_level16.txt','r',encoding='utf-8').readlines()\n",
    "level16=[i.strip().split('\\t')[0].replace('_',' ') for i in level16]\n",
    "\n",
    "level17=open('MWE_level17.txt','r',encoding='utf-8').readlines()\n",
    "level17=[i.strip().split('\\t')[0].replace('_',' ') for i in level17]\n",
    "\n",
    "level18=open('MWE_level18.txt','r',encoding='utf-8').readlines()\n",
    "level18=[i.strip().split('\\t')[0].replace('_',' ') for i in level18]\n",
    "\n",
    "level19=open('MWE_level19.txt','r',encoding='utf-8').readlines()\n",
    "level19=[i.strip().split('\\t')[0].replace('_',' ') for i in level19]\n",
    "\n",
    "level20=open('MWE_level20.txt','r',encoding='utf-8').readlines()\n",
    "level20=[i.strip().split('\\t')[0].replace('_',' ') for i in level20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go to',\n",
       " 'you know',\n",
       " 'i know',\n",
       " 'i mean',\n",
       " 'kind of',\n",
       " 'look at',\n",
       " 'do it',\n",
       " 'come on',\n",
       " 'get to',\n",
       " 'to go',\n",
       " 'more than',\n",
       " 'tell you',\n",
       " 'be on',\n",
       " 'of course',\n",
       " 'talk about',\n",
       " 'come to',\n",
       " 'go on',\n",
       " 'at least',\n",
       " 'look like',\n",
       " 'all of',\n",
       " 'a good',\n",
       " 'you see',\n",
       " 'use to',\n",
       " 'see you',\n",
       " 'sort of',\n",
       " 'get it',\n",
       " 'be really',\n",
       " 'come in',\n",
       " 'be use',\n",
       " 'end of',\n",
       " 'think about',\n",
       " 'in fact',\n",
       " 'so much',\n",
       " 'deal with',\n",
       " 'come back',\n",
       " 'come from',\n",
       " 'let go',\n",
       " 'what about',\n",
       " 'so what',\n",
       " 'at all',\n",
       " 'oh my god',\n",
       " 'think of',\n",
       " 'the second',\n",
       " 'find out',\n",
       " 'live in',\n",
       " 'as if',\n",
       " 'as well as',\n",
       " 'make sure',\n",
       " 'up to',\n",
       " 'excuse me',\n",
       " 'wait for',\n",
       " 'on it',\n",
       " 'feel like',\n",
       " 'work on',\n",
       " 'as well',\n",
       " 'in front of',\n",
       " 'in the world',\n",
       " 'with that',\n",
       " 'work for',\n",
       " 'get in',\n",
       " 'pick up',\n",
       " 'even if',\n",
       " 'will have',\n",
       " 'go out',\n",
       " 'for years',\n",
       " 'what if',\n",
       " 'you mean',\n",
       " 'go back to',\n",
       " 'focus on',\n",
       " 'turn out',\n",
       " 'take care of',\n",
       " 'pay for',\n",
       " 'not that',\n",
       " 'go through',\n",
       " 'be a good',\n",
       " 'come up',\n",
       " 'so far',\n",
       " 'no longer',\n",
       " 'grow up',\n",
       " 'what the hell',\n",
       " 'in order',\n",
       " 'come out',\n",
       " 'end up',\n",
       " 'get out',\n",
       " 'tend to',\n",
       " 'even though',\n",
       " 'set up',\n",
       " 'just say',\n",
       " 'go into',\n",
       " 'get into',\n",
       " 'in all',\n",
       " 'get out of',\n",
       " 'do in',\n",
       " 'relate to',\n",
       " 'at that',\n",
       " 'go in',\n",
       " 'ask for',\n",
       " 'no way',\n",
       " 'refer to',\n",
       " 'turn to',\n",
       " 'one day',\n",
       " 'as long as',\n",
       " 'how about',\n",
       " 'go and',\n",
       " 'very good',\n",
       " 'lead to',\n",
       " 'but then',\n",
       " 'why not',\n",
       " 'figure out',\n",
       " 'get back',\n",
       " 'show up',\n",
       " 'at the same time',\n",
       " 'call for',\n",
       " 'depend on',\n",
       " 'move to',\n",
       " 'as soon as',\n",
       " 'along with',\n",
       " 'get up',\n",
       " 'thank you for',\n",
       " 'go down',\n",
       " 'speak to',\n",
       " 'give up',\n",
       " 'what do you mean',\n",
       " 'sit down',\n",
       " 'hold on',\n",
       " 'go ahead',\n",
       " 'shut up',\n",
       " 'no matter',\n",
       " 'any more',\n",
       " 'do for',\n",
       " 'wake up',\n",
       " 'take place',\n",
       " 'point out',\n",
       " 'in terms of',\n",
       " 'as much as',\n",
       " 'result in',\n",
       " 'go with',\n",
       " 'oh yes',\n",
       " 'go back',\n",
       " 'work out',\n",
       " 'by the way',\n",
       " 'be go on',\n",
       " 'down to',\n",
       " 'go for',\n",
       " 'believe in',\n",
       " 'meet with',\n",
       " 'for one',\n",
       " 'put on',\n",
       " 'make up',\n",
       " 'take to',\n",
       " 'live with',\n",
       " 'come up with',\n",
       " 'very well',\n",
       " 'stay in',\n",
       " 'move on',\n",
       " 'as to',\n",
       " 'take over',\n",
       " 'the thing',\n",
       " 'as much',\n",
       " 'wait a minute',\n",
       " 'get on',\n",
       " 'in the middle of']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\gartl\\\\Documents\\\\MWE level list'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''find and bound phrases in whole corpus. \n",
    "this code bound longer phrases first because the number of longer phrases can be subsumed to the number of short phrases'''\n",
    "def phrase_extractor(lemma_sentences):\n",
    "    for i in range(len(lemma_sentences)):\n",
    "        for j in range(len(lemma_sentences[i])-5):\n",
    "            if lemma_sentences[i][j]+' '+lemma_sentences[i][j+1]+' '+lemma_sentences[i][j+2]+' '+lemma_sentences[i][j+3]+' '+lemma_sentences[i][j+4]+' '+lemma_sentences[i][j+5] in phrase:\n",
    "                lemma_sentences[i][j]=lemma_sentences[i][j]+'_'+lemma_sentences[i][j+1]+'_'+lemma_sentences[i][j+2]+'_'+lemma_sentences[i][j+3]+'_'+lemma_sentences[i][j+4]+'_'+lemma_sentences[i][j+5]\n",
    "                lemma_sentences[i][j+1] = ''\n",
    "                lemma_sentences[i][j+2] = ''\n",
    "                lemma_sentences[i][j+3] = ''\n",
    "                lemma_sentences[i][j+4] = ''\n",
    "                lemma_sentences[i][j+5] = ''\n",
    "            if lemma_sentences[i][j]+' '+lemma_sentences[i][j+1]+' '+lemma_sentences[i][j+2]+' '+lemma_sentences[i][j+3]+' '+lemma_sentences[i][j+4] in phrase:\n",
    "                lemma_sentences[i][j]=lemma_sentences[i][j]+'_'+lemma_sentences[i][j+1]+'_'+lemma_sentences[i][j+2]+'_'+lemma_sentences[i][j+3]+'_'+lemma_sentences[i][j+4]\n",
    "                lemma_sentences[i][j+1] = ''\n",
    "                lemma_sentences[i][j+2] = ''\n",
    "                lemma_sentences[i][j+3] = ''\n",
    "                lemma_sentences[i][j+4] = ''\n",
    "            if lemma_sentences[i][j]+' '+lemma_sentences[i][j+1]+' '+lemma_sentences[i][j+2]+' '+lemma_sentences[i][j+3] in phrase:\n",
    "                lemma_sentences[i][j]=lemma_sentences[i][j]+'_'+lemma_sentences[i][j+1]+'_'+lemma_sentences[i][j+2]+'_'+lemma_sentences[i][j+3]\n",
    "                lemma_sentences[i][j+1] = ''\n",
    "                lemma_sentences[i][j+2] = ''\n",
    "                lemma_sentences[i][j+3] = ''\n",
    "            if lemma_sentences[i][j]+' '+lemma_sentences[i][j+1]+' '+lemma_sentences[i][j+2] in phrase:\n",
    "                lemma_sentences[i][j]=lemma_sentences[i][j]+'_'+lemma_sentences[i][j+1]+'_'+lemma_sentences[i][j+2]\n",
    "                lemma_sentences[i][j+1] = ''\n",
    "                lemma_sentences[i][j+2] = ''\n",
    "            if lemma_sentences[i][j]+' '+lemma_sentences[i][j+1] in phrase:\n",
    "                lemma_sentences[i][j] = lemma_sentences[i][j] + '_' + lemma_sentences[i][j + 1]\n",
    "                lemma_sentences[i][j + 1] = ''\n",
    "        if len(lemma_sentences[i])>4 and lemma_sentences[i][-5]+' '+lemma_sentences[i][-4]+' '+lemma_sentences[i][-3]+' '+lemma_sentences[i][-2]+' '+lemma_sentences[i][-1] in phrase:\n",
    "            lemma_sentences[i][-5] = lemma_sentences[i][-5]+'_'+lemma_sentences[i][-4]+'_'+lemma_sentences[i][-3]+'_'+lemma_sentences[i][-2] + '_' + lemma_sentences[i][-1]\n",
    "            lemma_sentences[i][-4] = ''\n",
    "            lemma_sentences[i][-3] = ''\n",
    "            lemma_sentences[i][-2] = ''\n",
    "            lemma_sentences[i][-1] = ''\n",
    "        if len(lemma_sentences[i])>3 and lemma_sentences[i][-4]+' '+lemma_sentences[i][-3]+' '+lemma_sentences[i][-2]+' '+lemma_sentences[i][-1] in phrase:\n",
    "            lemma_sentences[i][-4] = lemma_sentences[i][-4]+'_'+lemma_sentences[i][-3]+'_'+lemma_sentences[i][-2] + '_' + lemma_sentences[i][-1]\n",
    "            lemma_sentences[i][-3] = ''\n",
    "            lemma_sentences[i][-2] =''\n",
    "            lemma_sentences[i][-1] = ''\n",
    "        if len(lemma_sentences[i])>2 and lemma_sentences[i][-3]+' '+lemma_sentences[i][-2]+' '+lemma_sentences[i][-1] in phrase:  ## to go 추가\n",
    "            lemma_sentences[i][-3] = lemma_sentences[i][-3]+'_'+lemma_sentences[i][-2] + '_' + lemma_sentences[i][-1]\n",
    "            lemma_sentences[i][-2] =''\n",
    "            lemma_sentences[i][-1] = ''\n",
    "        if len(lemma_sentences[i])>1 and lemma_sentences[i][-2]+' '+lemma_sentences[i][-1] in phrase:\n",
    "            lemma_sentences[i][-2] = lemma_sentences[i][-2] + '_' + lemma_sentences[i][-1]\n",
    "            lemma_sentences[i][-1] = ''\n",
    "        ''' remove every blanks('') in each sentences'''\n",
    "        while True:\n",
    "            if '' in lemma_sentences[i]:\n",
    "                lemma_sentences[i].remove('')\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return lemma_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''function for pre-processing'''\n",
    "def regularise(file_name):\n",
    "    sentence=[i.lower().strip().replace('\\ufeff','') for i in file_name]\n",
    "    clean_sentence=[clean_text(x) for x in sentence]\n",
    "    clean_sentences=[re.findall('\\w+',i) for i in clean_sentence]\n",
    "    return clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=[]\n",
    "for i in file_text:\n",
    "    files.append(regularise(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' lemmatize texts first then detect and bound phrases in texts'''\n",
    "def phrase_extraction(sentences):\n",
    "    clean_sentences = []\n",
    "    for i in sentences:\n",
    "        clean_sentence = []\n",
    "        for j in i:\n",
    "            clean_sentence.append(clean_text(j))\n",
    "        clean_sentences.append(clean_sentence)\n",
    "        \n",
    "    lemmatize_sentences=[]\n",
    "    for i in clean_sentences:\n",
    "        lemmatize_sentence=[]\n",
    "        for j in i:\n",
    "            lemmatize_sentence.append(wn.morphy(j))\n",
    "        lemmatize_sentences.append(lemmatize_sentence)\n",
    "\n",
    "    for i,y in zip(lemmatize_sentences,clean_sentences):\n",
    "        for j,x in enumerate(i):\n",
    "            if x==None:\n",
    "                i[j]=y[j]\n",
    "    \n",
    "    lemma_sentences=[]\n",
    "    for i in lemmatize_sentences:\n",
    "        lemma_sentence=[]\n",
    "        for j in i:\n",
    "            lemma_sentence.append(correction(j))\n",
    "        lemma_sentences.append(lemma_sentence)\n",
    "    \n",
    "    \n",
    "        \n",
    "    return phrase_extractor(lemma_sentences)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=[]\n",
    "for i in files:\n",
    "    file.append(phrase_extraction(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''3d list(word, sentence, file) -> 2d list (word, file) '''\n",
    "vocab_lists=[]\n",
    "for sentences in file:\n",
    "    vocab_list=[]\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            vocab_list.append(word)\n",
    "    vocab_lists.append(vocab_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=[]\n",
    "for sentences in file:\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            vocabulary.append(word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs=[vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['low',\n",
       " 'light',\n",
       " 'level',\n",
       " 'because',\n",
       " 'they',\n",
       " 'have',\n",
       " 'a',\n",
       " 'low',\n",
       " 'photosynthetic',\n",
       " 'rate']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_lists[-1][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['low',\n",
       " 'light',\n",
       " 'level',\n",
       " 'because',\n",
       " 'they',\n",
       " 'have',\n",
       " 'a',\n",
       " 'low',\n",
       " 'photosynthetic',\n",
       " 'rate']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in vocab_lists[0] if re.search('_',i)!=None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2311"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in vocabulary if re.search('_',i)!=None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/gartl/Documents/word level list')\n",
    "base1=open('word_level1.txt',encoding='utf-8').readlines()\n",
    "base1=[i.strip().split('\\t')[0].lower() for i in base1]\n",
    "\n",
    "base2=open('word_level2.txt',encoding='utf-8').readlines()\n",
    "base2=[i.strip().split('\\t')[0].lower() for i in base2]\n",
    "base2=[i for i in base2 if i not in base1]\n",
    "\n",
    "base3=open('word_level3.txt',encoding='utf-8').readlines()\n",
    "base3=[i.strip().split('\\t')[0].lower() for i in base3]\n",
    "base3=[i for i in base3 if i not in base1+base2]\n",
    "\n",
    "base4=open('word_level4.txt',encoding='utf-8').readlines()\n",
    "base4=[i.strip().split('\\t')[0].lower() for i in base4]\n",
    "base4=[i for i in base4 if i not in base1+base2+base3]\n",
    "\n",
    "base5=open('word_level5.txt',encoding='utf-8').readlines()\n",
    "base5=[i.strip().split('\\t')[0].lower() for i in base5]\n",
    "base5=[i for i in base5 if i not in base1+base2+base3+base4]\n",
    "\n",
    "base6=open('word_level6.txt',encoding='utf-8').readlines()\n",
    "base6=[i.strip().split('\\t')[0].lower() for i in base6]\n",
    "base6=[i for i in base6 if i not in base1+base2+base3+base4+base5]\n",
    "\n",
    "base7=open('word_level7.txt',encoding='utf-8').readlines()\n",
    "base7=[i.strip().split('\\t')[0].lower() for i in base7]\n",
    "base7=[i for i in base7 if i not in base1+base2+base3+base4+base5+base6]\n",
    "\n",
    "\n",
    "base8=open('word_level8.txt',encoding='utf-8').readlines()\n",
    "base8=[i.strip().split('\\t')[0].lower() for i in base8]\n",
    "base8=[i for i in base8 if i not in base1+base2+base3+base4+base5+base6+base7]\n",
    "\n",
    "base9=open('word_level9.txt',encoding='utf-8').readlines()\n",
    "base9=[i.strip().split('\\t')[0].lower() for i in base9]\n",
    "base9=[i for i in base9 if i not in base1+base2+base3+base4+base5+base6+base7+base8]\n",
    "\n",
    "\n",
    "base10=open('word_level10.txt',encoding='utf-8').readlines()\n",
    "base10=[i.strip().split('\\t')[0].lower() for i in base10]\n",
    "base10=[i for i in base10 if i not in base1+base2+base3+base4+base5+base6+base7+base8+base9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "904\n",
      "0\n",
      "904\n"
     ]
    }
   ],
   "source": [
    "print(len([i for i in base3 if i not in base1+base2]))\n",
    "print(len([i for i in base3 if i in base1+base2]))\n",
    "print(len(base3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' analyze a text difficulty by using MWE level list and BNC word level list'''\n",
    "class phrase_vocab_list:\n",
    "    def __init__(self,file):\n",
    "        self.file = file\n",
    "        \n",
    "    def phrase_level_list(self):\n",
    "        phrase_list=list([i for i in self.file if re.search('_',i)])\n",
    "    \n",
    "        phrase1=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level1]]\n",
    "        phrase2=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level2]]\n",
    "        phrase3=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level3]]\n",
    "        phrase4=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level4]]\n",
    "        phrase5=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level5]]\n",
    "        phrase6=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level6]]\n",
    "        phrase7=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level7]]\n",
    "        phrase8=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level8]]\n",
    "        phrase9=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level9]]\n",
    "        phrase10=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level10]]\n",
    "        phrase11=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level11]]\n",
    "        phrase12=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level12]]\n",
    "        phrase13=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level13]]\n",
    "        phrase14=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level14]]\n",
    "        phrase15=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level15]]\n",
    "        phrase16=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level16]]\n",
    "        phrase17=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level17]]\n",
    "        phrase18=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level18]]\n",
    "        phrase19=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level19]]\n",
    "        phrase20=[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level20]]\n",
    "    \n",
    "        return phrase1+phrase2+phrase3+phrase4+phrase5+phrase6+phrase7+phrase8+phrase9+phrase10+phrase11+phrase12+phrase13+phrase14+phrase15+phrase16+phrase17+phrase18+phrase19+phrase20\n",
    "    \n",
    "    def vocab_level_list(self):\n",
    "        vocab_list=list([i for i in self.file if re.search('_',i)==None])\n",
    "\n",
    "        vocab1=[[i for i in vocab_list if i in base1]]\n",
    "        vocab2=[[i for i in vocab_list if i in base2]]\n",
    "        vocab3=[[i for i in vocab_list if i in base3]]\n",
    "        vocab4=[[i for i in vocab_list if i in base4]]\n",
    "        vocab5=[[i for i in vocab_list if i in base5]]\n",
    "        vocab6=[[i for i in vocab_list if i in base6]]\n",
    "        vocab7=[[i for i in vocab_list if i in base7]]\n",
    "        vocab8=[[i for i in vocab_list if i in base8]]\n",
    "        vocab9=[[i for i in vocab_list if i in base9]]\n",
    "        vocab10=[[i for i in vocab_list if i in base10]]\n",
    "    \n",
    "        return vocab1+vocab2+vocab3+vocab4+vocab5+vocab6+vocab7+vocab8+vocab9+vocab10\n",
    "\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.phrase_level_list, self.vocab_level_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_level=[]\n",
    "#for i in vocab_lists:\n",
    "    #phrase_level.append(phrase_vocab_list(i)()[0])\n",
    "for i in vocab_lists:\n",
    "    phrase_level.append(phrase_vocab_list(i)()[0])\n",
    "    \n",
    "vocab_level=[]\n",
    "for i in vocab_lists:\n",
    "    vocab_level.append(phrase_vocab_list(i)()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_levels=[]\n",
    "#for i in vocab_lists:\n",
    "    #phrase_level.append(phrase_vocab_list(i)()[0])\n",
    "for i in vocabs:\n",
    "    phrase_levels.append(phrase_vocab_list(i)()[0])\n",
    "    \n",
    "vocab_levels=[]\n",
    "for i in vocabs:\n",
    "    vocab_levels.append(phrase_vocab_list(i)()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2165"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([j for i in phrase_levels[0]() for j in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1850\n",
      "2172\n"
     ]
    }
   ],
   "source": [
    "print(len([j for i in vocab_level[0]() for j in i]))\n",
    "print(len([i for i in vocab_lists[0] if re.search('_',i)==None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hold on',\n",
       " 'kind of',\n",
       " 'you know',\n",
       " 'you know',\n",
       " 'of course',\n",
       " 'i mean',\n",
       " 'be a good',\n",
       " 'at all',\n",
       " 'do it',\n",
       " 'wait a minute',\n",
       " 'i know',\n",
       " 'as long as',\n",
       " 'do it',\n",
       " 'of course',\n",
       " 'up to',\n",
       " 'at all',\n",
       " 'so what',\n",
       " 'be on',\n",
       " 'a good',\n",
       " 'get to',\n",
       " 'look at',\n",
       " 'of course',\n",
       " 'relate to',\n",
       " 'make sure',\n",
       " 'make sure',\n",
       " 'kind of',\n",
       " 'you know',\n",
       " 'kind of',\n",
       " 'in all',\n",
       " 'sort of',\n",
       " 'deal with',\n",
       " 'sort of',\n",
       " 'depend on',\n",
       " 'depend on',\n",
       " 'think about',\n",
       " 'look at',\n",
       " 'the second',\n",
       " 'sort of',\n",
       " 'will have',\n",
       " 'oh yes',\n",
       " 'come in',\n",
       " 'come in',\n",
       " 'sit down',\n",
       " 'very well',\n",
       " 'i know',\n",
       " 'as well',\n",
       " 'get to',\n",
       " 'be on',\n",
       " 'sort of',\n",
       " 'get to',\n",
       " 'wait for',\n",
       " 'go to',\n",
       " 'for years',\n",
       " 'use to',\n",
       " 'very good',\n",
       " 'tend to',\n",
       " 'be really',\n",
       " 'ask for',\n",
       " 'of course',\n",
       " 'see you',\n",
       " 'on it',\n",
       " 'pay for',\n",
       " 'as long as',\n",
       " 'to go',\n",
       " 'down to',\n",
       " 'sit down',\n",
       " 'the second',\n",
       " 'see you',\n",
       " 'end of',\n",
       " 'set up',\n",
       " 'to go',\n",
       " 'you know',\n",
       " 'in fact',\n",
       " 'in all',\n",
       " 'tend to',\n",
       " 'of course',\n",
       " 'lead to',\n",
       " 'look at',\n",
       " 'go into',\n",
       " 'the second',\n",
       " 'to go',\n",
       " 'up to',\n",
       " 'of course',\n",
       " 'come to',\n",
       " 'of course',\n",
       " 'wake up',\n",
       " 'come on',\n",
       " 'what about',\n",
       " 'what about',\n",
       " 'live with',\n",
       " 'what about',\n",
       " 'go on',\n",
       " 'talk about',\n",
       " 'make sure',\n",
       " 'be really',\n",
       " 'be really',\n",
       " 'tend to',\n",
       " 'look at',\n",
       " 'do it',\n",
       " 'look at',\n",
       " 'be a good',\n",
       " 'you mean',\n",
       " 'use to',\n",
       " 'of course',\n",
       " 'you see',\n",
       " 'so what',\n",
       " 'tend to',\n",
       " 'at least',\n",
       " 'think about',\n",
       " 'thank you for',\n",
       " 'at the same time',\n",
       " 'give up',\n",
       " 'i know',\n",
       " 'talk about',\n",
       " 'at the same time',\n",
       " 'look at',\n",
       " 'so what',\n",
       " 'at least',\n",
       " 'make sure',\n",
       " 'you see',\n",
       " 'as much as',\n",
       " 'be really',\n",
       " 'excuse me',\n",
       " 'be on',\n",
       " 'at least',\n",
       " 'be on',\n",
       " 'go to',\n",
       " 'what about',\n",
       " 'as well',\n",
       " 'to go',\n",
       " 'come back',\n",
       " 'come back',\n",
       " 'wait for',\n",
       " 'get to',\n",
       " 'get to',\n",
       " 'go to',\n",
       " 'go back to',\n",
       " 'to go',\n",
       " 'find out',\n",
       " 'sit down',\n",
       " 'of course',\n",
       " 'depend on',\n",
       " 'go to',\n",
       " 'be really',\n",
       " 'up to',\n",
       " 'pay for',\n",
       " 'find out',\n",
       " 'use to',\n",
       " 'get to',\n",
       " 'do for',\n",
       " 'look at',\n",
       " 'you know',\n",
       " 'sit down',\n",
       " 'as much',\n",
       " 'more than',\n",
       " 'talk about',\n",
       " 'of course',\n",
       " 'up to',\n",
       " 'look like',\n",
       " 'i know',\n",
       " 'find out',\n",
       " 'so what',\n",
       " 'in fact',\n",
       " 'refer to',\n",
       " 'think of',\n",
       " 'look at',\n",
       " 'relate to',\n",
       " 'sort of',\n",
       " 'but then',\n",
       " 'look at',\n",
       " 'use to',\n",
       " 'go to',\n",
       " 'look at',\n",
       " 'look at',\n",
       " 'sort of',\n",
       " 'at least',\n",
       " 'look at',\n",
       " 'end of',\n",
       " 'any more',\n",
       " 'at that',\n",
       " 'come to',\n",
       " 'you know',\n",
       " 'as well',\n",
       " 'tell you',\n",
       " 'in fact',\n",
       " 'point out',\n",
       " 'be on',\n",
       " 'a good',\n",
       " 'end of',\n",
       " 'to go',\n",
       " 'move on',\n",
       " 'be use',\n",
       " 'come back',\n",
       " 'end of',\n",
       " 'come to',\n",
       " 'a good',\n",
       " 'end of',\n",
       " 'sort of',\n",
       " 'turn to',\n",
       " 'go on',\n",
       " 'so what',\n",
       " 'as if',\n",
       " 'depend on',\n",
       " 'you see',\n",
       " 'think about',\n",
       " 'you see',\n",
       " 'you mean',\n",
       " 'do it',\n",
       " 'so much',\n",
       " 'depend on',\n",
       " 'be use',\n",
       " 'look at',\n",
       " 'end of',\n",
       " 'end of',\n",
       " 'be use',\n",
       " 'of course',\n",
       " 'more than',\n",
       " 'oh yes',\n",
       " 'you know',\n",
       " 'i mean',\n",
       " 'at all',\n",
       " 'but then',\n",
       " 'in front of',\n",
       " 'oh yes',\n",
       " 'talk about',\n",
       " 'tell you',\n",
       " 'at all',\n",
       " 'live in',\n",
       " 'find out',\n",
       " 'tell you',\n",
       " 'kind of',\n",
       " 'do it',\n",
       " 'as well as',\n",
       " 'be on',\n",
       " 'of course',\n",
       " 'come from',\n",
       " 'look at',\n",
       " 'look at',\n",
       " 'i mean',\n",
       " 'oh yes',\n",
       " 'do it',\n",
       " 'oh yes',\n",
       " 'what about',\n",
       " 'as if',\n",
       " 'talk about',\n",
       " 'as well',\n",
       " 'i mean',\n",
       " 'look at',\n",
       " 'get to',\n",
       " 'look at',\n",
       " 'deal with',\n",
       " 'what about',\n",
       " 'you know',\n",
       " 'get to',\n",
       " 'at least',\n",
       " 'as well',\n",
       " 'at the same time',\n",
       " 'take over',\n",
       " 'make sure',\n",
       " 'i mean',\n",
       " 'so much',\n",
       " 'talk about',\n",
       " 'of course',\n",
       " 'live in',\n",
       " 'relate to',\n",
       " 'sort of',\n",
       " 'turn to',\n",
       " 'go to',\n",
       " 'more than',\n",
       " 'as if',\n",
       " 'move on',\n",
       " 'end of',\n",
       " 'move to',\n",
       " 'at the same time',\n",
       " 'move on',\n",
       " 'oh yes',\n",
       " 'you know',\n",
       " 'on it',\n",
       " 'do it',\n",
       " 'so what',\n",
       " 'at least',\n",
       " 'look at',\n",
       " 'be on',\n",
       " 'for years',\n",
       " 'come to',\n",
       " 'get to',\n",
       " 'come in',\n",
       " 'oh yes',\n",
       " 'come in',\n",
       " 'see you',\n",
       " 'at least',\n",
       " 'you know',\n",
       " 'very well',\n",
       " 'you know',\n",
       " 'i mean',\n",
       " 'work on',\n",
       " 'as soon as',\n",
       " 'go to',\n",
       " 'focus on',\n",
       " 'tell you',\n",
       " 'go through',\n",
       " 'not that',\n",
       " 'go and',\n",
       " 'all of',\n",
       " 'all of',\n",
       " 'will have',\n",
       " 'come from',\n",
       " 'of course',\n",
       " 'a good',\n",
       " 'stay in',\n",
       " 'go and',\n",
       " 'oh yes',\n",
       " 'you mean',\n",
       " 'of course',\n",
       " 'oh yes',\n",
       " 'very well',\n",
       " 'tell you',\n",
       " 'tell you',\n",
       " 'make sure',\n",
       " 'i know',\n",
       " 'you know',\n",
       " 'look like',\n",
       " 'of course',\n",
       " 'come up with',\n",
       " 'tell you',\n",
       " 'what about',\n",
       " 'to go',\n",
       " 'one day',\n",
       " 'to go',\n",
       " 'depend on',\n",
       " 'at least',\n",
       " 'oh yes',\n",
       " 'use to',\n",
       " 'find out',\n",
       " 'a good',\n",
       " 'come up with',\n",
       " 'you see',\n",
       " 'you know',\n",
       " 'of course',\n",
       " 'of course',\n",
       " 'the second',\n",
       " 'of course',\n",
       " 'oh yes',\n",
       " 'you know',\n",
       " 'look at',\n",
       " 'of course',\n",
       " 'go for',\n",
       " 'work on',\n",
       " 'end of',\n",
       " 'you know',\n",
       " 'get it',\n",
       " 'look like',\n",
       " 'be really',\n",
       " 'you know',\n",
       " 'more than',\n",
       " 'at the same time',\n",
       " 'of course',\n",
       " 'come from',\n",
       " 'more than',\n",
       " 'up to',\n",
       " 'very good',\n",
       " 'see you',\n",
       " 'ask for',\n",
       " 'oh yes',\n",
       " 'use to',\n",
       " 'ask for',\n",
       " 'use to',\n",
       " 'you know',\n",
       " 'ask for',\n",
       " 'in fact',\n",
       " 'do it',\n",
       " 'up to',\n",
       " 'oh yes',\n",
       " 'at that',\n",
       " 'come back',\n",
       " 'even though',\n",
       " 'no longer',\n",
       " 'move to',\n",
       " 'come from',\n",
       " 'look at',\n",
       " 'come to',\n",
       " 'as well as',\n",
       " 'the second',\n",
       " 'a good',\n",
       " 'use to',\n",
       " 'come to',\n",
       " 'use to',\n",
       " 'up to',\n",
       " 'end of',\n",
       " 'look at',\n",
       " 'look at',\n",
       " 'look like',\n",
       " 'at least',\n",
       " 'look like',\n",
       " 'a good',\n",
       " 'i mean',\n",
       " 'so what',\n",
       " 'for one',\n",
       " 'how about',\n",
       " 'kind of',\n",
       " 'you know',\n",
       " 'what about',\n",
       " 'you know',\n",
       " 'of course',\n",
       " 'you mean',\n",
       " 'so what',\n",
       " 'do it',\n",
       " 'move on',\n",
       " 'come from',\n",
       " 'you know',\n",
       " 'come from',\n",
       " 'what about',\n",
       " 'at that',\n",
       " 'be use',\n",
       " 'use to',\n",
       " 'be really',\n",
       " 'as well as',\n",
       " 'come from',\n",
       " 'be on',\n",
       " 'even though',\n",
       " 'look like',\n",
       " 'be a good',\n",
       " 'kind of',\n",
       " 'come to',\n",
       " 'see you',\n",
       " 'be use',\n",
       " 'of course',\n",
       " 'refer to',\n",
       " 'of course',\n",
       " 'you see',\n",
       " 'be on',\n",
       " 'end of',\n",
       " 'look at',\n",
       " 'look at',\n",
       " 'be really',\n",
       " 'as well as',\n",
       " 'as well',\n",
       " 'make up',\n",
       " 'a good',\n",
       " 'move on',\n",
       " 'be use',\n",
       " 'what do you mean',\n",
       " 'be use',\n",
       " 'look at',\n",
       " 'what about',\n",
       " 'i know',\n",
       " 'look at',\n",
       " 'be use',\n",
       " 'to go',\n",
       " 'a good',\n",
       " 'get to',\n",
       " 'of course',\n",
       " 'come in',\n",
       " 'hold on',\n",
       " 'kind of',\n",
       " 'you know',\n",
       " 'you know',\n",
       " 'of course',\n",
       " 'i mean',\n",
       " 'be a good',\n",
       " 'at all',\n",
       " 'do it',\n",
       " 'wait a minute',\n",
       " 'i know',\n",
       " 'as long as',\n",
       " 'do it',\n",
       " 'of course',\n",
       " 'up to',\n",
       " 'at all',\n",
       " 'so what',\n",
       " 'be on',\n",
       " 'a good',\n",
       " 'get to',\n",
       " 'look at',\n",
       " 'of course',\n",
       " 'relate to',\n",
       " 'make sure',\n",
       " 'make sure',\n",
       " 'kind of',\n",
       " 'you know',\n",
       " 'kind of',\n",
       " 'in all',\n",
       " 'sort of',\n",
       " 'deal with',\n",
       " 'sort of',\n",
       " 'depend on',\n",
       " 'depend on',\n",
       " 'think about',\n",
       " 'look at',\n",
       " 'the second',\n",
       " 'sort of',\n",
       " 'will have',\n",
       " 'oh yes',\n",
       " 'come in',\n",
       " 'come in',\n",
       " 'sit down',\n",
       " 'very well',\n",
       " 'i know',\n",
       " 'as well',\n",
       " 'get to',\n",
       " 'be on',\n",
       " 'sort of',\n",
       " 'get to',\n",
       " 'wait for',\n",
       " 'go to',\n",
       " 'for years',\n",
       " 'use to',\n",
       " 'very good',\n",
       " 'tend to',\n",
       " 'be really',\n",
       " 'ask for',\n",
       " 'of course',\n",
       " 'see you',\n",
       " 'on it',\n",
       " 'pay for',\n",
       " 'as long as',\n",
       " 'to go',\n",
       " 'down to',\n",
       " 'sit down',\n",
       " 'the second',\n",
       " 'see you',\n",
       " 'end of',\n",
       " 'set up',\n",
       " 'to go',\n",
       " 'you know',\n",
       " 'in fact',\n",
       " 'in all',\n",
       " 'tend to',\n",
       " 'of course',\n",
       " 'lead to',\n",
       " 'look at',\n",
       " 'go into',\n",
       " 'the second',\n",
       " 'to go',\n",
       " 'up to',\n",
       " 'of course',\n",
       " 'come to',\n",
       " 'of course',\n",
       " 'wake up',\n",
       " 'come on',\n",
       " 'what about',\n",
       " 'what about',\n",
       " 'live with',\n",
       " 'what about',\n",
       " 'go on',\n",
       " 'talk about',\n",
       " 'make sure',\n",
       " 'be really',\n",
       " 'be really',\n",
       " 'tend to',\n",
       " 'look at',\n",
       " 'do it',\n",
       " 'look at',\n",
       " 'be a good',\n",
       " 'you mean',\n",
       " 'use to',\n",
       " 'of course',\n",
       " 'you see',\n",
       " 'so what',\n",
       " 'tend to',\n",
       " 'at least',\n",
       " 'think about',\n",
       " 'thank you for',\n",
       " 'at the same time',\n",
       " 'give up',\n",
       " 'i know',\n",
       " 'talk about',\n",
       " 'at the same time',\n",
       " 'look at',\n",
       " 'so what',\n",
       " 'at least',\n",
       " 'make sure',\n",
       " 'you see',\n",
       " 'as much as',\n",
       " 'be really',\n",
       " 'excuse me',\n",
       " 'be on',\n",
       " 'at least',\n",
       " 'be on',\n",
       " 'go to',\n",
       " 'what about',\n",
       " 'as well',\n",
       " 'to go',\n",
       " 'come back',\n",
       " 'come back',\n",
       " 'wait for',\n",
       " 'get to',\n",
       " 'get to',\n",
       " 'go to',\n",
       " 'go back to',\n",
       " 'to go',\n",
       " 'find out',\n",
       " 'sit down',\n",
       " 'of course',\n",
       " 'depend on',\n",
       " 'go to',\n",
       " 'be really',\n",
       " 'up to',\n",
       " 'pay for',\n",
       " 'find out',\n",
       " 'use to',\n",
       " 'get to',\n",
       " 'do for',\n",
       " 'look at',\n",
       " 'you know',\n",
       " 'sit down',\n",
       " 'as much',\n",
       " 'more than',\n",
       " 'talk about',\n",
       " 'of course',\n",
       " 'up to',\n",
       " 'look like',\n",
       " 'i know',\n",
       " 'find out',\n",
       " 'so what',\n",
       " 'in fact',\n",
       " 'refer to',\n",
       " 'think of',\n",
       " 'look at',\n",
       " 'relate to',\n",
       " 'sort of',\n",
       " 'but then',\n",
       " 'look at',\n",
       " 'use to',\n",
       " 'go to',\n",
       " 'look at',\n",
       " 'look at',\n",
       " 'sort of',\n",
       " 'at least',\n",
       " 'look at',\n",
       " 'end of',\n",
       " 'any more',\n",
       " 'at that',\n",
       " 'come to',\n",
       " 'you know',\n",
       " 'as well',\n",
       " 'tell you',\n",
       " 'in fact',\n",
       " 'point out',\n",
       " 'be on',\n",
       " 'a good',\n",
       " 'end of',\n",
       " 'to go',\n",
       " 'move on',\n",
       " 'be use',\n",
       " 'come back',\n",
       " 'end of',\n",
       " 'come to',\n",
       " 'a good',\n",
       " 'end of',\n",
       " 'sort of',\n",
       " 'turn to',\n",
       " 'go on',\n",
       " 'so what',\n",
       " 'as if',\n",
       " 'depend on',\n",
       " 'you see',\n",
       " 'think about',\n",
       " 'you see',\n",
       " 'you mean',\n",
       " 'do it',\n",
       " 'so much',\n",
       " 'depend on',\n",
       " 'be use',\n",
       " 'look at',\n",
       " 'end of',\n",
       " 'end of',\n",
       " 'be use',\n",
       " 'of course',\n",
       " 'more than',\n",
       " 'oh yes',\n",
       " 'you know',\n",
       " 'i mean',\n",
       " 'at all',\n",
       " 'but then',\n",
       " 'in front of',\n",
       " 'oh yes',\n",
       " 'talk about',\n",
       " 'tell you',\n",
       " 'at all',\n",
       " 'live in',\n",
       " 'find out',\n",
       " 'tell you',\n",
       " 'kind of',\n",
       " 'do it',\n",
       " 'as well as',\n",
       " 'be on',\n",
       " 'of course',\n",
       " 'come from',\n",
       " 'look at',\n",
       " 'look at',\n",
       " 'i mean',\n",
       " 'oh yes',\n",
       " 'do it',\n",
       " 'oh yes',\n",
       " 'what about',\n",
       " 'as if',\n",
       " 'talk about',\n",
       " 'as well',\n",
       " 'i mean',\n",
       " 'look at',\n",
       " 'get to',\n",
       " 'look at',\n",
       " 'deal with',\n",
       " 'what about',\n",
       " 'you know',\n",
       " 'get to',\n",
       " 'at least',\n",
       " 'as well',\n",
       " 'at the same time',\n",
       " 'take over',\n",
       " 'make sure',\n",
       " 'i mean',\n",
       " 'so much',\n",
       " 'talk about',\n",
       " 'of course',\n",
       " 'live in',\n",
       " 'relate to',\n",
       " 'sort of',\n",
       " 'turn to',\n",
       " 'go to',\n",
       " 'more than',\n",
       " 'as if',\n",
       " 'move on',\n",
       " 'end of',\n",
       " 'move to',\n",
       " 'at the same time',\n",
       " 'move on',\n",
       " 'oh yes',\n",
       " 'you know',\n",
       " 'on it',\n",
       " 'do it',\n",
       " 'so what',\n",
       " 'at least',\n",
       " 'look at',\n",
       " 'be on',\n",
       " 'for years',\n",
       " 'come to',\n",
       " 'get to',\n",
       " 'come in',\n",
       " 'oh yes',\n",
       " 'come in',\n",
       " 'see you',\n",
       " 'at least',\n",
       " 'you know',\n",
       " 'very well',\n",
       " 'you know',\n",
       " 'i mean',\n",
       " 'work on',\n",
       " 'as soon as',\n",
       " 'go to',\n",
       " 'focus on',\n",
       " 'tell you',\n",
       " 'go through',\n",
       " 'not that',\n",
       " 'go and',\n",
       " 'all of',\n",
       " 'all of',\n",
       " 'will have',\n",
       " 'come from',\n",
       " 'of course',\n",
       " 'a good',\n",
       " 'stay in',\n",
       " 'go and',\n",
       " 'oh yes',\n",
       " 'you mean',\n",
       " 'of course',\n",
       " 'oh yes',\n",
       " 'very well',\n",
       " 'tell you',\n",
       " 'tell you',\n",
       " 'make sure',\n",
       " 'i know',\n",
       " 'you know',\n",
       " 'look like',\n",
       " 'of course',\n",
       " 'come up with',\n",
       " 'tell you',\n",
       " 'what about',\n",
       " 'to go',\n",
       " 'one day',\n",
       " 'to go',\n",
       " 'depend on',\n",
       " 'at least',\n",
       " 'oh yes',\n",
       " 'use to',\n",
       " 'find out',\n",
       " 'a good',\n",
       " 'come up with',\n",
       " 'you see',\n",
       " 'you know',\n",
       " 'of course',\n",
       " 'of course',\n",
       " 'the second',\n",
       " 'of course',\n",
       " 'oh yes',\n",
       " 'you know',\n",
       " 'look at',\n",
       " 'of course',\n",
       " 'go for',\n",
       " 'work on',\n",
       " 'end of',\n",
       " 'you know',\n",
       " 'get it',\n",
       " 'look like',\n",
       " 'be really',\n",
       " 'you know',\n",
       " 'more than',\n",
       " 'at the same time',\n",
       " 'of course',\n",
       " 'come from',\n",
       " 'more than',\n",
       " 'up to',\n",
       " 'very good',\n",
       " 'of course',\n",
       " 'what about',\n",
       " 'i know',\n",
       " 'as well as',\n",
       " 'of course',\n",
       " 'what about',\n",
       " 'kind of',\n",
       " 'come in',\n",
       " 'thank you for',\n",
       " 'go in',\n",
       " 'use to',\n",
       " 'use to',\n",
       " 'go and',\n",
       " 'use to',\n",
       " 'so what',\n",
       " 'use to',\n",
       " 'to go',\n",
       " 'in fact',\n",
       " 'use to',\n",
       " 'one day',\n",
       " 'what about',\n",
       " 'by the way',\n",
       " 'you know',\n",
       " 'as well as',\n",
       " 'oh yes',\n",
       " 'of course',\n",
       " 'look at',\n",
       " 'focus on',\n",
       " 'come to',\n",
       " 'in the world',\n",
       " 'live in',\n",
       " 'live in',\n",
       " 'be use',\n",
       " 'of course',\n",
       " 'go on',\n",
       " 'see you',\n",
       " 'ask for',\n",
       " 'oh yes',\n",
       " 'use to',\n",
       " 'ask for',\n",
       " 'use to',\n",
       " 'you know',\n",
       " 'ask for',\n",
       " 'in fact',\n",
       " 'do it',\n",
       " 'up to',\n",
       " 'oh yes',\n",
       " 'at that',\n",
       " 'come back',\n",
       " 'even though',\n",
       " 'no longer',\n",
       " 'move to',\n",
       " 'come from',\n",
       " 'look at',\n",
       " 'come to',\n",
       " 'as well as',\n",
       " 'the second',\n",
       " 'a good',\n",
       " 'use to',\n",
       " 'come to',\n",
       " 'use to',\n",
       " 'up to',\n",
       " 'end of',\n",
       " 'look at',\n",
       " 'look at',\n",
       " 'look like',\n",
       " 'at least',\n",
       " 'look like',\n",
       " 'a good',\n",
       " 'i mean',\n",
       " 'so what',\n",
       " 'for one',\n",
       " 'how about',\n",
       " 'kind of',\n",
       " 'you know',\n",
       " 'what about',\n",
       " 'you know',\n",
       " 'of course',\n",
       " 'you mean',\n",
       " 'so what',\n",
       " 'do it',\n",
       " 'move on',\n",
       " 'come from',\n",
       " 'you know',\n",
       " 'come from',\n",
       " 'what about',\n",
       " 'at that',\n",
       " 'be use',\n",
       " 'use to',\n",
       " 'be really',\n",
       " 'as well as',\n",
       " 'come from',\n",
       " 'be on',\n",
       " 'even though',\n",
       " 'look like',\n",
       " 'in the world',\n",
       " 'more than',\n",
       " 'use to',\n",
       " 'in terms of',\n",
       " 'give up',\n",
       " 'depend on',\n",
       " 'be use',\n",
       " 'more than',\n",
       " 'more than',\n",
       " 'more than',\n",
       " 'in order',\n",
       " 'even if',\n",
       " 'live in',\n",
       " 'all of',\n",
       " 'more than',\n",
       " 'very well',\n",
       " 'kind of',\n",
       " 'figure out',\n",
       " 'come up with',\n",
       " 'at least',\n",
       " 'kind of',\n",
       " 'as well',\n",
       " 'relate to',\n",
       " 'relate to',\n",
       " 'go with',\n",
       " 'in fact',\n",
       " 'do it',\n",
       " 'go on',\n",
       " 'take place',\n",
       " 'in the world',\n",
       " 'in the world',\n",
       " 'of course',\n",
       " 'the second',\n",
       " 'be a good',\n",
       " 'for years',\n",
       " 'all of',\n",
       " 'in order',\n",
       " 'as soon as',\n",
       " 'go out',\n",
       " 'find out',\n",
       " 'come from',\n",
       " 'use to',\n",
       " 'use to',\n",
       " 'come from',\n",
       " 'be a good',\n",
       " 'at the same time',\n",
       " 'kind of',\n",
       " 'be use',\n",
       " 'be use',\n",
       " 'as well as',\n",
       " 'go into',\n",
       " 'at least',\n",
       " 'more than',\n",
       " 'as much',\n",
       " 'end of',\n",
       " 'in fact',\n",
       " 'at the same time',\n",
       " 'be on',\n",
       " 'you know',\n",
       " 'more than',\n",
       " 'no longer',\n",
       " 'more than',\n",
       " 'will have',\n",
       " 'the second',\n",
       " 'more than',\n",
       " 'work for',\n",
       " 'in terms of',\n",
       " 'more than',\n",
       " 'as well as',\n",
       " 'as well as',\n",
       " 'put on',\n",
       " 'sort of',\n",
       " 'come to',\n",
       " 'work for',\n",
       " 'in terms of',\n",
       " 'more than',\n",
       " 'come from',\n",
       " 'use to',\n",
       " 'use to',\n",
       " 'come from',\n",
       " 'be a good',\n",
       " 'at the same time',\n",
       " 'kind of',\n",
       " 'be use',\n",
       " 'be use',\n",
       " 'as well as',\n",
       " 'go into',\n",
       " 'take to',\n",
       " 'end of',\n",
       " 'the second',\n",
       " 'as well',\n",
       " ...]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_levels[0]()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''show the statistics'''\n",
    "def text_stats(file_num): #list return version\n",
    "    num_phrase='phrase: {:f}%'.format((len([j for i in phrase_level[file_num]() for j in i])/len(vocab_lists[file_num]))*100)\n",
    "    num_vocab='vocab: {:f}%'.format((len([j for i in vocab_level[file_num]() for j in i])/len(vocab_lists[file_num]))*100)\n",
    "    P1='phrase 1: {:f}%'.format((len(phrase_level[file_num]()[0])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P2='phrase 2: {:f}%'.format((len(phrase_level[file_num]()[1])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P3='phrase 3: {:f}%'.format((len(phrase_level[file_num]()[2])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P4='phrase 4: {:f}%'.format((len(phrase_level[file_num]()[3])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P5='phrase 5: {:f}%'.format((len(phrase_level[file_num]()[4])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P6='phrase 6: {:f}%'.format((len(phrase_level[file_num]()[5])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P7='phrase 7: {:f}%'.format((len(phrase_level[file_num]()[6])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P8='phrase 8: {:f}%'.format((len(phrase_level[file_num]()[7])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P9='phrase 9: {:f}%'.format((len(phrase_level[file_num]()[8])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P10='phrase 10: {:f}%'.format((len(phrase_level[file_num]()[9])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P11='phrase 11: {:f}%'.format((len(phrase_level[file_num]()[10])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P12='phrase 12: {:f}%'.format((len(phrase_level[file_num]()[11])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P13='phrase 13: {:f}%'.format((len(phrase_level[file_num]()[12])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P14='phrase 14: {:f}%'.format((len(phrase_level[file_num]()[13])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P15='phrase 15: {:f}%'.format((len(phrase_level[file_num]()[14])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P16='phrase 16: {:f}%'.format((len(phrase_level[file_num]()[15])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P17='phrase 17: {:f}%'.format((len(phrase_level[file_num]()[16])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P18='phrase 18: {:f}%'.format((len(phrase_level[file_num]()[17])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P19='phrase 19: {:f}%'.format((len(phrase_level[file_num]()[18])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    P20='phrase 20: {:f}%'.format((len(phrase_level[file_num]()[19])/len([j for i in phrase_level[file_num]() for j in i]))*100)\n",
    "    \n",
    "    L1='vocab 1: {:f}%'.format((len(vocab_level[file_num]()[0])/len([j for i in vocab_level[file_num]() for j in i]))*100)\n",
    "    L2='vocab 2: {:f}%'.format((len(vocab_level[file_num]()[1])/len([j for i in vocab_level[file_num]() for j in i]))*100)\n",
    "    L3='vocab 3: {:f}%'.format((len(vocab_level[file_num]()[2])/len([j for i in vocab_level[file_num]() for j in i]))*100)\n",
    "    L4='vocab 4: {:f}%'.format((len(vocab_level[file_num]()[3])/len([j for i in vocab_level[file_num]() for j in i]))*100)\n",
    "    L5='vocab 5: {:f}%'.format((len(vocab_level[file_num]()[4])/len([j for i in vocab_level[file_num]() for j in i]))*100)\n",
    "    L6='vocab 6: {:f}%'.format((len(vocab_level[file_num]()[5])/len([j for i in vocab_level[file_num]() for j in i]))*100)\n",
    "    L7='vocab 7: {:f}%'.format((len(vocab_level[file_num]()[6])/len([j for i in vocab_level[file_num]() for j in i]))*100)\n",
    "    L8='vocab 8: {:f}%'.format((len(vocab_level[file_num]()[7])/len([j for i in vocab_level[file_num]() for j in i]))*100)\n",
    "    L9='vocab 9: {:f}%'.format((len(vocab_level[file_num]()[8])/len([j for i in vocab_level[file_num]() for j in i]))*100)\n",
    "    L10='vocab 10: {:f}%'.format((len(vocab_level[file_num]()[9])/len([j for i in vocab_level[file_num]() for j in i]))*100)\n",
    "    \n",
    "    return num_phrase,num_vocab,P1,P2,P3,P4,P5,P6,P7,P8,P9,P10,P11,P12,P13,P14,P15,P16,P17,P18,P19,P20,L1,L2,L3,L4,L5,L6,L7,L8,L9,L10\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''show the statistics'''\n",
    "def stats(file_num):  # list return version\n",
    "    num_phrase = 'phrase: {:f}%'.format(\n",
    "        (len([j for i in phrase_levels[file_num]() for j in i]) / len(vocabs[file_num])) * 100)\n",
    "    num_vocab = 'vocab: {:f}%'.format(\n",
    "        (len([j for i in vocab_levels[file_num]() for j in i]) / len(vocabs[file_num])) * 100)\n",
    "    P1 = 'phrase 1: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[0]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P2 = 'phrase 2: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[1]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P3 = 'phrase 3: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[2]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P4 = 'phrase 4: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[3]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P5 = 'phrase 5: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[4]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P6 = 'phrase 6: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[5]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P7 = 'phrase 7: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[6]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P8 = 'phrase 8: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[7]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P9 = 'phrase 9: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[8]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P10 = 'phrase 10: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[9]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P11 = 'phrase 11: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[10]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P12 = 'phrase 12: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[11]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P13 = 'phrase 13: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[12]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P14 = 'phrase 14: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[13]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P15 = 'phrase 15: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[14]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P16 = 'phrase 16: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[15]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P17 = 'phrase 17: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[16]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P18 = 'phrase 18: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[17]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P19 = 'phrase 19: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[18]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "    P20 = 'phrase 20: {:f}%'.format(\n",
    "        (len(phrase_levels[file_num]()[19]) / len([j for i in phrase_levels[file_num]() for j in i])) * 100)\n",
    "\n",
    "    L1 = 'vocab 1: {:f}%'.format(\n",
    "        (len(vocab_levels[file_num]()[0]) / len([j for i in vocab_levels[file_num]() for j in i])) * 100)\n",
    "    L2 = 'vocab 2: {:f}%'.format(\n",
    "        (len(vocab_levels[file_num]()[1]) / len([j for i in vocab_levels[file_num]() for j in i])) * 100)\n",
    "    L3 = 'vocab 3: {:f}%'.format(\n",
    "        (len(vocab_levels[file_num]()[2]) / len([j for i in vocab_levels[file_num]() for j in i])) * 100)\n",
    "    L4 = 'vocab 4: {:f}%'.format(\n",
    "        (len(vocab_levels[file_num]()[3]) / len([j for i in vocab_levels[file_num]() for j in i])) * 100)\n",
    "    L5 = 'vocab 5: {:f}%'.format(\n",
    "        (len(vocab_levels[file_num]()[4]) / len([j for i in vocab_levels[file_num]() for j in i])) * 100)\n",
    "    L6 = 'vocab 6: {:f}%'.format(\n",
    "        (len(vocab_levels[file_num]()[5]) / len([j for i in vocab_levels[file_num]() for j in i])) * 100)\n",
    "    L7 = 'vocab 7: {:f}%'.format(\n",
    "        (len(vocab_levels[file_num]()[6]) / len([j for i in vocab_levels[file_num]() for j in i])) * 100)\n",
    "    L8 = 'vocab 8: {:f}%'.format(\n",
    "        (len(vocab_levels[file_num]()[7]) / len([j for i in vocab_levels[file_num]() for j in i])) * 100)\n",
    "    L9 = 'vocab 9: {:f}%'.format(\n",
    "        (len(vocab_levels[file_num]()[8]) / len([j for i in vocab_levels[file_num]() for j in i])) * 100)\n",
    "    L10 = 'vocab 10: {:f}%'.format(\n",
    "        (len(vocab_levels[file_num]()[9]) / len([j for i in vocab_levels[file_num]() for j in i])) * 100)\n",
    "\n",
    "    return num_phrase, num_vocab, P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15, P16, P17, P18, P19, P20, L1, L2, L3, L4, L5, L6, L7, L8, L9, L10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('phrase: 2.306209%',\n",
       " 'vocab: 84.361452%',\n",
       " 'phrase 1: 58.614319%',\n",
       " 'phrase 2: 14.734411%',\n",
       " 'phrase 3: 8.498845%',\n",
       " 'phrase 4: 4.203233%',\n",
       " 'phrase 5: 3.510393%',\n",
       " 'phrase 6: 2.401848%',\n",
       " 'phrase 7: 2.586605%',\n",
       " 'phrase 8: 1.062356%',\n",
       " 'phrase 9: 0.785219%',\n",
       " 'phrase 10: 0.877598%',\n",
       " 'phrase 11: 0.646651%',\n",
       " 'phrase 12: 0.415704%',\n",
       " 'phrase 13: 0.138568%',\n",
       " 'phrase 14: 0.646651%',\n",
       " 'phrase 15: 0.369515%',\n",
       " 'phrase 16: 0.092379%',\n",
       " 'phrase 17: 0.092379%',\n",
       " 'phrase 18: 0.000000%',\n",
       " 'phrase 19: 0.138568%',\n",
       " 'phrase 20: 0.184758%',\n",
       " 'vocab 1: 79.295671%',\n",
       " 'vocab 2: 7.894338%',\n",
       " 'vocab 3: 3.841103%',\n",
       " 'vocab 4: 2.424365%',\n",
       " 'vocab 5: 1.964746%',\n",
       " 'vocab 6: 1.316986%',\n",
       " 'vocab 7: 1.034143%',\n",
       " 'vocab 8: 1.108642%',\n",
       " 'vocab 9: 0.632608%',\n",
       " 'vocab 10: 0.487398%')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ielts_stat=[]\n",
    "for i in range(len(vocabs)):\n",
    "    ielts_stat.append(stats(i))\n",
    "\n",
    "ielts_stat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('phrase: 2.154472%',\n",
       " 'vocab: 90.934959%',\n",
       " 'phrase 1: 86.792453%',\n",
       " 'phrase 2: 9.433962%',\n",
       " 'phrase 3: 0.000000%',\n",
       " 'phrase 4: 1.886792%',\n",
       " 'phrase 5: 0.000000%',\n",
       " 'phrase 6: 1.886792%',\n",
       " 'vocab 1: 88.690210%',\n",
       " 'vocab 2: 6.124274%',\n",
       " 'vocab 3: 3.352705%',\n",
       " 'vocab 4: 1.162271%',\n",
       " 'vocab 5: 0.670541%')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat=[]\n",
    "for i in range(len(vocab_lists)):\n",
    "    stat.append(text_stats(i))\n",
    "\n",
    "stat[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''mark different level of phrases by using different symbol'''\n",
    "def changephrase(file):\n",
    "    phrase_list=list(set([i for t in file for i in t if re.search('_',i)]))\n",
    "    \n",
    "    phrase1=[i for i in phrase_list if i.replace('_',' ') in level1]\n",
    "    phrase2=[i for i in phrase_list if i.replace('_',' ') in level2]\n",
    "    phrase3=[i for i in phrase_list if i.replace('_',' ') in level3]\n",
    "    phrase4=[i for i in phrase_list if i.replace('_',' ') in level4]\n",
    "    phrase5=[i for i in phrase_list if i.replace('_',' ') in level5]\n",
    "    phrase6=[i for i in phrase_list if i.replace('_',' ') in level6]\n",
    "    phrase7=[i for i in phrase_list if i.replace('_',' ') in level7]\n",
    "    phrase8=[i for i in phrase_list if i.replace('_',' ') in level8]\n",
    "    phrase9=[i for i in phrase_list if i.replace('_',' ') in level9]\n",
    "    phrase10=[i for i in phrase_list if i.replace('_',' ') in level10]\n",
    "    phrase11=[i for i in phrase_list if i.replace('_',' ') in level11]\n",
    "    phrase12=[i for i in phrase_list if i.replace('_',' ') in level12]\n",
    "    phrase13=[i for i in phrase_list if i.replace('_',' ') in level13]\n",
    "    phrase14=[i for i in phrase_list if i.replace('_',' ') in level14]\n",
    "    phrase15=[i for i in phrase_list if i.replace('_',' ') in level15]\n",
    "    phrase16=[i for i in phrase_list if i.replace('_',' ') in level16]\n",
    "    phrase17=[i for i in phrase_list if i.replace('_',' ') in level17]\n",
    "    phrase18=[i for i in phrase_list if i.replace('_',' ') in level18]\n",
    "    phrase19=[i for i in phrase_list if i.replace('_',' ') in level19]\n",
    "    phrase20=[i for i in phrase_list if i.replace('_',' ') in level20]\n",
    "  \n",
    "    sentences=[]\n",
    "    for i in file:\n",
    "        sentence=[]\n",
    "        for j in i:\n",
    "            if j in phrase1:\n",
    "                sentence.append(re.sub('_','@',j))\n",
    "            if j in phrase2: \n",
    "                sentence.append(re.sub('_','%',j))\n",
    "            if j in phrase3:\n",
    "                sentence.append(re.sub('_','__',j)) \n",
    "            if j in phrase4:\n",
    "                sentence.append(re.sub('_','~',j))\n",
    "            if j in phrase5:\n",
    "                sentence.append(re.sub('_','!',j))\n",
    "            if j in phrase6:\n",
    "                sentence.append(re.sub('_','=',j))\n",
    "            \n",
    "\n",
    "            else:\n",
    "                sentence.append(j)\n",
    "        sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            if j in phrase7:\n",
    "                sentence.append(re.sub('_','@@',j))\n",
    "            if j in phrase8:\n",
    "                sentence.append(re.sub('_','%%',j))\n",
    "            if j in phrase9:\n",
    "                sentence.append(re.sub('_','~~',j))\n",
    "            if j in phrase10:\n",
    "                sentence.append(re.sub('_','!!',j))\n",
    "            if j in phrase11:\n",
    "                sentence.append(re.sub('_','==',j))\n",
    "            if j in phrase12: \n",
    "                sentence.append(re.sub('_','%%%',j))\n",
    "            if j in phrase13:\n",
    "                sentence.append(re.sub('_','___',j)) \n",
    "            if j in phrase14:\n",
    "                sentence.append(re.sub('_','~~~',j))\n",
    "            if j in phrase15:\n",
    "                sentence.append(re.sub('_','!!!',j))\n",
    "            if j in phrase16:\n",
    "                sentence.append(re.sub('_','===',j))\n",
    "            if j in phrase17:\n",
    "                sentence.append(re.sub('_','@@@',j))\n",
    "            if j in phrase18:\n",
    "                sentence.append(re.sub('_','%@',j))\n",
    "            if j in phrase19:\n",
    "                sentence.append(re.sub('_','~@',j))\n",
    "            if j in phrase20:\n",
    "                sentence.append(re.sub('_','!@',j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''remove words that contain '_' symbol even though they are not phrase '''\n",
    "def removephrase(file):\n",
    "    sentences=[]\n",
    "    for i in file:\n",
    "        sentence=[]\n",
    "        for j in i:\n",
    "            if re.search('_',j)==None:\n",
    "                sentence.append(j)\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_filed=[]\n",
    "for i in file:\n",
    "    original_filed.append(changephrase(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_file=[]\n",
    "for i in original_filed:\n",
    "    original_file.append(removephrase(i))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''lemmatized version'''\n",
    "original_files=[]\n",
    "for i in original_file:\n",
    "    original_sentences=[]\n",
    "    for j in i:\n",
    "        if len(j)>0:\n",
    "            original_sentences.append(' '.join(j))\n",
    "    original_files.append(original_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''original version'''\n",
    "correct_files=[]\n",
    "for i in files:\n",
    "    correct_file=[]\n",
    "    for j in i:\n",
    "        if len(j)>0:\n",
    "            correct_file.append(' '.join(j))\n",
    "    correct_files.append(correct_file)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''find the location of phrases in lemmatized sentences and bound the words in equivalent position in original corpus'''\n",
    "def unlemmatizer(original_file,correct_file):\n",
    "    result=[]\n",
    "    for index1, sentence in enumerate(original_file):\n",
    "        words = [i for i in sentence.split(' ')]\n",
    "        new_sentence = correct_file[index1].split(' ')\n",
    "        for index2, i in enumerate(words):\n",
    "            if '@' in i:\n",
    "                #print(i.split('@'))\n",
    "                new_sentence[index2]='@'.join(new_sentence[index2:index2+len(i.split('@'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('@'))]\n",
    "            if '%' in i:\n",
    "                #print(i.split('%'))\n",
    "                new_sentence[index2]='%'.join(new_sentence[index2:index2+len(i.split('%'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('%'))]\n",
    "            if '__' in i:\n",
    "                new_sentence[index2]='__'.join(new_sentence[index2:index2+len(i.split('__'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('__'))]\n",
    "            if '~' in i:\n",
    "                new_sentence[index2]='~'.join(new_sentence[index2:index2+len(i.split('~'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('~'))]\n",
    "            if '!' in i:\n",
    "                new_sentence[index2]='!'.join(new_sentence[index2:index2+len(i.split('!'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('!'))]\n",
    "            if '=' in i:\n",
    "                new_sentence[index2]='='.join(new_sentence[index2:index2+len(i.split('='))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('='))]     \n",
    "            if '@@' in i:\n",
    "                #print(i.split('@'))\n",
    "                new_sentence[index2]='@@'.join(new_sentence[index2:index2+len(i.split('@@'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('@@'))]\n",
    "            \n",
    "            \n",
    "\n",
    "        result.append(' '.join(new_sentence))\n",
    "        \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "            if '@@' in i:\n",
    "                #print(i.split('@'))\n",
    "                new_sentence[index2]='@@'.join(new_sentence[index2:index2+len(i.split('@@'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('@@'))]\n",
    "            if '%%' in i:\n",
    "                #print(i.split('%'))\n",
    "                new_sentence[index2]='%%'.join(new_sentence[index2:index2+len(i.split('%%'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('%%'))]\n",
    "            if '~~' in i:\n",
    "                new_sentence[index2]='~~'.join(new_sentence[index2:index2+len(i.split('~~'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('~~'))]\n",
    "            if '!!' in i:\n",
    "                new_sentence[index2]='!!'.join(new_sentence[index2:index2+len(i.split('!!'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('!!'))]\n",
    "            if '==' in i:\n",
    "                new_sentence[index2]='=='.join(new_sentence[index2:index2+len(i.split('=='))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('=='))]\n",
    "            if '%%%' in i:\n",
    "                new_sentence[index2]='%%%'.join(new_sentence[index2:index2+len(i.split('%%%'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('%%%'))]\n",
    "            if '___' in i:\n",
    "                #print(i.split('@'))\n",
    "                new_sentence[index2]='___'.join(new_sentence[index2:index2+len(i.split('___'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('___'))]\n",
    "            if '~~~' in i:\n",
    "                #print(i.split('%'))\n",
    "                new_sentence[index2]='~~~'.join(new_sentence[index2:index2+len(i.split('~~~'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('~~~'))]\n",
    "            if '!!!' in i:\n",
    "                new_sentence[index2]='!!!'.join(new_sentence[index2:index2+len(i.split('!!!'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('!!!'))]\n",
    "            if '===' in i:\n",
    "                new_sentence[index2]='==='.join(new_sentence[index2:index2+len(i.split('==='))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('==='))]\n",
    "            if '@@@' in i:\n",
    "                new_sentence[index2]='@@@'.join(new_sentence[index2:index2+len(i.split('@@@'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('@@@'))]\n",
    "            if '%@' in i:\n",
    "                new_sentence[index2]='%@'.join(new_sentence[index2:index2+len(i.split('%@'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('%@'))]\n",
    "            if '~@' in i:\n",
    "                new_sentence[index2]='~@'.join(new_sentence[index2:index2+len(i.split('~@'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('~@'))]\n",
    "            if '!@' in i:\n",
    "                new_sentence[index2]='!@'.join(new_sentence[index2:index2+len(i.split('!@'))])\n",
    "                del new_sentence[index2+1:index2+len(i.split('!@'))]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['section 1',\n",
       " 'louise oh hello i d like to join the video library',\n",
       " 'mr max ok would%you%like to fill~in the application form now',\n",
       " 'louise yes i can do it now',\n",
       " 'mr max hold@on and i ll get a form now i ll just ask you a few questions and then i ll get you to sign at the bottom',\n",
       " 'louise right',\n",
       " 'mr max what s your full name',\n",
       " 'louise louise cynthia jones example',\n",
       " 'mr max jones',\n",
       " 'louise yes that s right',\n",
       " 'repeat',\n",
       " 'mr max ok and what s your address',\n",
       " 'louise apartment 1 72 black street highbridge',\n",
       " 'mr max black street that s just around the corner isn t it',\n",
       " 'louise yes',\n",
       " 'mr max ok so the post code is 2085 right',\n",
       " 'louise yes 2085',\n",
       " 'mr max mm and your telephone number i need both home and work',\n",
       " 'louise home is 9835 6712 and work is 9456 1309 do you need any id or anything like that',\n",
       " 'mr max yes we need your driver s licence number that is if you have one',\n",
       " 'louise yes 1 know if off by heart it s an easy one 2020bd do you need to see it',\n",
       " 'mr max yes iam afraid i do',\n",
       " 'louise mm here',\n",
       " 'mr max right thanks and could you tell me your date of birth please',\n",
       " 'louise 25 july 1977',\n",
       " 'mr max that s the most important part out of the way but could i just ask you a few questions for a survey weare conducting',\n",
       " 'louise ok',\n",
       " 'mr max what kind@of videos do you prefer to watch have a look at this list',\n",
       " 'louise well i love anything that makes me laugh i just love to hear jokes and funny punch lines iam not very keen on westerns although my father likes them but iam a real softie so anything with a bit of a love story is good for me it doesn t matter how old not musicals though theyare too much',\n",
       " 'mr max anything else',\n",
       " 'louise iam completely taken by documentaries of the great outdoors you@know the sort animals plants and far away places 1 saw a wonderful one on dolphins last week it was amazing',\n",
       " 'mr max now i think that s all from me except i need you to sign here on~the~line here s a pen oh and i nearly forgot the membership fee 25 refundable if you leave the library for any reason',\n",
       " 'louise there you are and do i sign here',\n",
       " 'mr max yes that s it you can borrow videos now if you like but your card won t be ready until next week you can~come~and pick it up when you bring your first videos back that is if you want to take some now',\n",
       " 'louise yes i d like to i ll have a look%around',\n",
       " 'mr max fine',\n",
       " 'section 2',\n",
       " 'interviewer a dream came true in 1995 when over 96 days of the spring and summer an expedition of four men undertook what they believe to have been the first and only complete end to end crossing of morocco s attora mountains i talked to charles owen the leader of the expedition group about the trip',\n",
       " 'charles how much planning went on beforehand',\n",
       " 'charles well as you@know i run these walking trips across the mountains for tourists and over the years i ve collected maps and other data to prepare what i call a route book for this trip and this book basically shows the route across the mountains that we took',\n",
       " 'interviewer you actually broke records while you were out there didn t you',\n",
       " 'charles mmm yes it was 900 miles in total and we managed to climb 32 peaks that were over 3000 metres high including toubkal which is of@course the highest in north africa we weren t actually out to make a name for ourselves it just happened really',\n",
       " 'interviewer what was the weather like',\n",
       " 'charles it got us right from day one and we were pretty taken aback really to find that it rained on quite a number of days and so we were forced to start re planning our route almost from the outset one of the obvious problems is the heavy snow which blocks the mountain passes so you have to make considerable detours when we were on the way to imilchil for example the snow forced us into a northern bypass which was new to us but anyway either way we would have been rewarded because we fell upon amazing high meadows huge gorges and wonderful snow capped mountains the scenery was as fine as any we saw on the trip and that was how it was every time having to take another pass was never a disappointment',\n",
       " 'charles yes yes we d arranged to meet up with friends at various points on the',\n",
       " 'journey i@mean this was actually one of the purposes of the trip and we managed to keep all these dates which is amazing really considering the detours we made an old friend acted as a sort of transport organiser for everyone and the hotel ali@in@marrakech was a good social base i d really recommend it although i can t remember who runs it anyway groups of friends actually joined us for three week stints and others just linked up with us some whom we hadn t@met before the trip at all tagged on for short bursts people from the area who just came along for the ride but outside the major visitor areas like toubkal we only met one other group of travellers like ourselves in the whole 96 days',\n",
       " 'interviewer were there any bad moments',\n",
       " 'charles we took two i!must!say long suffering donkeys with us to help transport water and tents and things i suppose if we were to do@it all again we d probably hire donkeys along~the~way taza and tamri as we called them after the last places in the trip well they made quite a unique journey between them and but it was continuously demanding for them on both the really high summits they took diversions that were quite out of character and i can only assume that it must have been due to tiredness',\n",
       " 'interviewer well thank you and charles has put together a video about this journey and continues to lead groups to the attora mountains so if you want further information',\n",
       " 'section 3',\n",
       " 'jane hi tim tim jane how are you tim fine i d been wondering when i d run%into you have you been here long',\n",
       " 'tim i arrived yesterday on sunday how about you',\n",
       " 'jane i got here a few days ago on saturday no wait@a@minute what s today sorry friday not saturday',\n",
       " 'tim but we didn t have to be here till today',\n",
       " 'jane yes i@know but i wanted to get my things moved into my room and just take a look%around so did you decide to do english in%the%end',\n",
       " 'tim no i changed my mind and opted for history instead and youare doing biology if i remember correctly',\n",
       " 'jane yes although to=start=with i couldn t decide between that and geography',\n",
       " 'tim how much reading have you got i was given an amazingly long list of books to read see',\n",
       " 'jane wow it does look pretty long',\n",
       " 'tim well i counted 57 i could hardly believe it what s your list like',\n",
       " 'jane well it s not as@long@as yours but it s still pretty big there are 43 i don t know how iam going to get through them all',\n",
       " 'tim well you don t have to read them all this week you just have to stay ahead of the lectures and seminars have you got your class schedule yet',\n",
       " 'jane yep it came with the reading list when s your first lecture',\n",
       " 'tim tuesday how about you',\n",
       " 'jane the day after it s my busiest day i ve got two lectures in the morning and one in the afternoon',\n",
       " 'jane it s going to be different from school isn t it',\n",
       " 'tim yeah particularly the lectures have you got any special strategy for listening to lectures',\n",
       " 'jane well iam going to use a cassette recorder and record them all',\n",
       " 'tim what are you allowed to',\n",
       " 'jane sure lots of people do@it nowadays it means you can listen to the lectures all over',\n",
       " 'again later and make really good notes',\n",
       " 'tim i couldn t do that i like to take notes as iam listening i usually find i get all the important points reading is different of@course my approach is to skim the book first to see what s important and what isn t it saves hours of time',\n",
       " 'jane but what if you miss something',\n",
       " 'tim you don t mean youare going to read every word do you',\n",
       " 'jane well that s what i usually do',\n",
       " 'tim well that s up@to you but i think youare crazy',\n",
       " 'jane what s your first lecture on anyway',\n",
       " 'tim oh it s a lecture on the french revolution',\n",
       " 'jane the french revolution how boring',\n",
       " 'tim it s not boring at@all it was an amazing period of history it changed everything in',\n",
       " 'europe so@what s your first lecture about',\n",
       " 'jane it s about animal behaviour it sounds really interesting',\n",
       " 'tim look i was@on my way to the library iam going to get some of these books out and start reading for the first essay i ve got to write',\n",
       " 'jane and what have you got to write about',\n",
       " 'tim well you ll never believe it i think our professor must have a sense of humour he s given us the title why study history',\n",
       " 'jane that s a@good one when you find the answer let me know',\n",
       " 'tim iam going to enjoy writing it have you been given any writing assignments yet',\n",
       " 'jane yes i ve got@to write about animal language',\n",
       " 'tim hmm that sounds a challenge i suppose you ll be off to the zoo to do field research',\n",
       " 'section 4',\n",
       " 'lecturer',\n",
       " 'welcome to further education information week this is the physical education faculty s session and iam the head of the faculty during the course of this morning we hope to give you a clear idea of what we offer in our training programs and we will look@at the types of@courses and the entry requirements if any for those courses some of these courses are open to school leavers but for some you need previous qualifications or relevant successful employment',\n",
       " 'so firstly the physical fitness instructor s course is offered as a six month certificate course which includes an important component of personal fitness but there are no specific entry requirements',\n",
       " 'for sports administrators we provide a four month certificate course but you should be aware that this is designed for those who are in employment this employment must be current and related@to sports administration for the sports psychologist course we offer a one year diploma course but this diploma course is available only to those who already hold a degree in psychology so you need to make@sure you have that before you apply to do this course',\n",
       " 'for the sports psychologist course we offer a one year diploma course but this diploma course is available only to those who already hold a degree in psychology so you need to make@sure you have that before you apply to do this course',\n",
       " 'now for physical education teachers we offer a four year degree in education this degree course is designed for preparing students to teach in primary and secondary schools and needs no prior qualifications as%it%is entered directly by school leavers and lastly for the recreation officer s course we offer a six month certificate entry to this course normally includes applicants of a wide range of ages and experiences but we do not insist on any prerequisites for this course',\n",
       " 'remember that this is a vocational training institute we train you so that you can take%up a particular kind@of job so it is important that you@know the main roles of the jobs what the work is like and what kind@of qualities you need to succeed at them',\n",
       " 'a physical fitness instructor works in health and fitness centres preparing individual programs for ordinary members of the public physical fitness instructors prepare routines of exercises to suit the individual client s age and level of fitness',\n",
       " 'sports administrators run clubs and sporting associations their duties include such things as booking playing fields with local councils and organising the schedule of games or events for the club so they need good organisational skills',\n",
       " 'sports psychologists spend time with professional athletes helping them approach competition with a positive mental attitude to enable them to achieve their personal best they do this by improving motivation and concentration or assisting with stress management',\n",
       " 'physical education or pe teachers instruct young students in how to exercise play sport and do other recreational activities correctly and safely pe teachers help the development of co ordination balance posture and flexibility with things like simple catching and throwing skills they are not expected to be experts in@all sports but must be able to show students the basic techniques involved in a wide range of activities',\n",
       " 'recreation officers often find themselves working for local government authorities and local groups their aim is to raise people s awareness of healthy lifestyles and improved general fitness through arranging recreational activities for groups of all ages from the very young to the elderly',\n",
       " 'there are many other job opportunities which our graduates can look%forward%to if you are interested in any of these']"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlemmatizer(original_files[0], correct_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlemmatize_file=[]\n",
    "for x,y in zip(original_files, correct_files):\n",
    "    unlemmatize_file.append(unlemmatizer(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''make each string as a sentence'''\n",
    "unlemmatize_files=[]\n",
    "for i in unlemmatize_file:\n",
    "    unlemmatize_sentences=[x for y in i for x in y.split()]\n",
    "    unlemmatize_files.append(unlemmatize_sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''eliminate symbols that are used to classify phrases into each level and show the result'''\n",
    "class phrase_vocab:\n",
    "    def __init__(self,file):\n",
    "        self.file = file\n",
    "        \n",
    "    def phrase_level_list(self):\n",
    "        \n",
    "        phrase1=[list(set([i.replace('@',' ') for i in self.file if re.search('@',i)!=None]))]\n",
    "        phrase2=[list(set([i.replace('%',' ') for i in self.file if re.search('%',i)!=None]))]\n",
    "        phrase3=[list(set([i.replace('__',' ') for i in self.file if re.search('__',i)!=None]))]\n",
    "        phrase4=[list(set([i.replace('~',' ') for i in self.file if re.search('~',i)!=None]))]\n",
    "        phrase5=[list(set([i.replace('!',' ') for i in self.file if re.search('!',i)!=None]))]\n",
    "        phrase6=[list(set([i.replace('=',' ') for i in self.file if re.search(r'=',i)!=None]))]\n",
    "        #phrase7=[list(set([i.replace('@@',' ') for i in self.file if re.search(r'@@',i)!=None]))]\n",
    "        #[[i.replace('_',' ') for i in phrase_list if i.replace('_',' ') in level7]]\n",
    "        \n",
    "        return phrase1+phrase2+phrase3+phrase4+phrase5+phrase6\n",
    "    \n",
    "    def vocab_level_list(self):\n",
    "        vocab_list=list([i for i in self.file if i !=''])\n",
    "\n",
    "        vocab1=[list(set([i for i in vocab_list if i in base1]))]\n",
    "        vocab2=[list(set([i for i in vocab_list if i in base2]))]\n",
    "        vocab3=[list(set([i for i in vocab_list if i in base3]))]\n",
    "        vocab4=[list(set([i for i in vocab_list if i in base4]))]\n",
    "        vocab5=[list(set([i for i in vocab_list if i in base5]))]\n",
    "    \n",
    "        return vocab1+vocab2+vocab3+vocab4+vocab5\n",
    "\n",
    "    \n",
    "    def __call__(self):\n",
    "        return self.phrase_level_list(), self.vocab_level_list()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_level=[]\n",
    "for i in unlemmatize_files:\n",
    "    phrases_level.append(phrase_vocab(i)()[0])\n",
    "    \n",
    "vocabs_level=[]\n",
    "for i in unlemmatize_files:\n",
    "    vocabs_level.append(phrase_vocab(i)()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'phrases_level' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5713556ab280>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mphrases_level\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'phrases_level' is not defined"
     ]
    }
   ],
   "source": [
    "phrases_level[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
